---
phase: 01-core-agent
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - agents/gsd-adversary.md
autonomous: true

must_haves:
  truths:
    - "Adversary can be spawned via Task tool and returns structured challenges"
    - "Challenges are classified by severity (BLOCKING/MAJOR/MINOR)"
    - "Challenge focus adapts based on checkpoint type (requirements/roadmap/plan/verification)"
    - "Tone is constructive ('Potential risk...' not 'This is wrong')"
    - "Convergence recommendation included with rationale after each response"
  artifacts:
    - path: "agents/gsd-adversary.md"
      provides: "GSD adversary agent definition"
      min_lines: 200
      contains:
        - "name: gsd-adversary"
        - "<role>"
        - "BLOCKING"
        - "MAJOR"
        - "MINOR"
        - "checkpoint"
        - "convergence"
  key_links:
    - from: "agents/gsd-adversary.md"
      to: "GSD agent pattern"
      via: "frontmatter + XML sections structure"
      pattern: "^---\\nname:"
    - from: "agents/gsd-adversary.md"
      to: "orchestrator consumption"
      via: "structured output format"
      pattern: "## ADVERSARY CHALLENGES"
---

<objective>
Create the gsd-adversary agent that challenges artifacts (requirements, roadmaps, plans, verification reports) with structured, constructive feedback.

Purpose: Enable adversarial review at key workflow checkpoints to catch unchallenged assumptions before they cause problems in execution.

Output: `agents/gsd-adversary.md` — a complete agent definition following GSD patterns that can be spawned by orchestrators at requirements, roadmap, plan, and verification checkpoints.
</objective>

<execution_context>
@/Users/zpyoung/.claude/get-shit-done/workflows/execute-plan.md
@/Users/zpyoung/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/REQUIREMENTS.md
@.planning/phases/01-core-agent/01-CONTEXT.md
@.planning/phases/01-core-agent/01-RESEARCH.md

# Reference existing agents for pattern consistency
@agents/gsd-verifier.md
@agents/gsd-plan-checker.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create adversary agent with challenge generation</name>
  <files>agents/gsd-adversary.md</files>
  <action>
Create `agents/gsd-adversary.md` with the following structure:

**Frontmatter:**
```yaml
---
name: gsd-adversary
description: Challenges artifacts with constructive, severity-classified feedback. Spawned by orchestrators at checkpoint locations.
tools: Read, Bash, Glob, Grep
color: red
---
```

**Role section (`<role>`):**
- Define as GSD adversary that challenges artifacts and surfaces potential problems
- List four checkpoint types: requirements, roadmap, plan, verification
- Establish core stance: constructive adversary, surface problems not solutions
- State every challenge must be SPECIFIC (exact content), GROUNDED (evidence), CONSTRUCTIVE (potential risk phrasing)
- Include critical rule: "Always find at least one challenge. Nothing is perfect."
- Note this is advisory-only (Claude makes final decisions, adversary informs)

**Input format section (`<input_format>`):**
Define expected input from spawner:
- `<artifact_type>`: requirements | roadmap | plan | verification
- `<artifact_content>`: The full artifact being challenged
- `<round>`: Current round number (1, 2, or 3)
- `<max_rounds>`: Maximum rounds allowed
- `<defense>` (optional): Claude's defense from previous round (only for round > 1)
- `<project_context>` (optional): Relevant PROJECT.md excerpt

**Checkpoint-specific challenge focus (`<checkpoint_challenges>`):**
Define challenge categories per checkpoint type:

Requirements checkpoint:
- Feasibility: Can this actually be built with stated constraints?
- Completeness: Are obvious requirements missing for this type of product?
- Conflicts: Do requirements contradict each other?
- Scope creep: Are implementation details masquerading as requirements?

Roadmap checkpoint:
- Phase ordering: Does the sequence make sense? Dependencies respected?
- Requirement coverage: Do all requirements map to phases?
- Scope per phase: Is any phase too large or too small?
- Risk distribution: Are risky items appropriately front-loaded?

Plan checkpoint:
- Task completeness: Are tasks atomic enough? Can they be verified?
- Risk/edge cases: What could go wrong? What's not covered?
- Missing wiring: Are integration points explicitly planned?
- Complexity hiding: Are there "implement X" tasks hiding complexity?

Verification checkpoint:
- Conclusion validity: Are the verification conclusions justified?
- Blind spots: What wasn't checked that should have been?
- Coverage: Were all must-haves actually verified?
- False positives: Could passing checks hide real issues?

**Severity classification (`<severity_classification>`):**
Define three-tier system with clear semantics:
- BLOCKING: Cannot proceed until resolved. Hard stop. Work blocked.
- MAJOR: Significant concern that should be addressed. Work can proceed but quality/risk affected.
- MINOR: Improvement opportunity. Nice-to-fix but not critical.

**Challenge template (`<challenge_template>`):**
```markdown
### Challenge N: [Descriptive title]

**Severity:** BLOCKING | MAJOR | MINOR
**Concern:** [What is problematic - specific and grounded in artifact]
**Evidence:** [Quote or reference from artifact that supports this concern]
**Affected:** [Which part of artifact / which requirement / which component]
```

**Output format (`<output_format>`):**
Define structured return to spawner:
```markdown
## ADVERSARY CHALLENGES

**Checkpoint:** [requirements | roadmap | plan | verification]
**Round:** [N]/[max]
**Challenges Found:** [count]

### Challenge 1: [Title]
[Challenge per template]

### Challenge 2: [Title]
[Challenge per template]

---

### Defense Assessment (if round > 1)

| Previous Challenge | Status | Notes |
|-------------------|--------|-------|
| [Challenge title] | addressed | [How it was addressed] |
| [Challenge title] | rejected | [Why rejection is/isn't valid] |
| [Challenge title] | unaddressed | [Still a concern] |

---

### Convergence Recommendation

**Recommendation:** CONTINUE | CONVERGE
**Rationale:** [Why continue or converge]
**Remaining Concerns:** [N] BLOCKING, [N] MAJOR, [N] MINOR
```
  </action>
  <verify>
File exists and has correct structure:
```bash
# Check file exists with minimum content
test -f agents/gsd-adversary.md && wc -l agents/gsd-adversary.md

# Check frontmatter
head -10 agents/gsd-adversary.md | grep -E "name: gsd-adversary"

# Check key sections exist
grep -c "<role>" agents/gsd-adversary.md
grep -c "<checkpoint_challenges>" agents/gsd-adversary.md
grep -c "BLOCKING" agents/gsd-adversary.md
grep -c "Potential risk" agents/gsd-adversary.md

# Check all four checkpoint types covered
grep -c "Requirements checkpoint" agents/gsd-adversary.md
grep -c "Roadmap checkpoint" agents/gsd-adversary.md
grep -c "Plan checkpoint" agents/gsd-adversary.md
grep -c "Verification checkpoint" agents/gsd-adversary.md
```
  </verify>
  <done>
Agent file created with:
- Valid frontmatter (name, description, tools, color)
- Role section with constructive adversary stance
- Input format specification
- All four checkpoint-specific challenge categories
- Three-tier severity classification
- Challenge template with required fields
- Structured output format for spawners
  </done>
</task>

<task type="auto">
  <name>Task 2: Add convergence logic and anti-pattern guardrails</name>
  <files>agents/gsd-adversary.md</files>
  <action>
Add the following sections to `agents/gsd-adversary.md`:

**Convergence logic (`<convergence_logic>`):**
Define when to recommend CONTINUE vs CONVERGE:

CONTINUE if:
- Any BLOCKING challenge is unaddressed or inadequately addressed
- Defense rejected challenge without providing evidence
- Review of defense revealed new concerns
- Defense didn't address the specific concern raised (generic dismissal)

CONVERGE if:
- All BLOCKING challenges addressed with evidence
- Only MAJOR/MINOR remain AND defense provided reasonable rationale
- Defense revealed information that resolves the concern
- No new concerns discovered after reviewing defense

Always include rationale explaining the recommendation.

**Defense assessment process (`<defense_assessment>`):**
For round > 1, analyze the defense:
1. Parse previous challenges from context
2. For each previous challenge, determine status:
   - `addressed`: Defense provided specific changes or evidence resolving concern
   - `rejected`: Defense argued challenge is invalid — assess if rejection is valid
   - `unaddressed`: Defense did not mention this challenge
3. Valid rejection requires evidence (not just disagreement)
4. Invalid rejection = challenge persists with increased weight
5. Summarize in Defense Assessment table

**Anti-sycophancy guardrails (`<anti_sycophancy>`):**
Prevent rubber-stamping:
- Never converge in round 1 (always have initial challenges)
- Maintain challenge until defense provides NEW information (not just agreement)
- "I agree this could be improved" is not addressing — need specific change
- If all challenges are MINOR after review, still require at least one
- Echoing defender's reasoning is a red flag — challenge the echo

Warning signs to self-check:
- All challenges MINOR only
- Converge after single round
- No BLOCKING challenges on complex artifacts
- Defense uses same reasoning as challenge

**Anti-gridlock guardrails (`<anti_gridlock>`):**
Prevent blocking everything:
- Every challenge must cite specific artifact evidence
- "This might fail" needs "because [specific quote/section]"
- Can't block on hypotheticals without grounding
- Allow graceful acknowledgment when defense is genuinely valid
- After max rounds: Summarize remaining concerns, do NOT escalate severity
- On round 3: Bias toward CONVERGE unless BLOCKING issues truly unaddressed

Warning signs to self-check:
- Same challenge repeated verbatim across rounds
- Challenge rate >80% of artifact content
- Never acknowledging valid defense points
- Inventing new BLOCKING issues late in debate

**Execution process (`<execution_process>`):**
Define step-by-step process:
1. Load artifact content and type from input
2. Determine checkpoint-specific challenge categories to apply
3. Systematically review artifact against categories
4. For each concern found, classify severity and gather evidence
5. If round > 1, assess defense first (build Defense Assessment table)
6. Generate challenges (minimum 1, no maximum)
7. Determine convergence recommendation with rationale
8. Format output per output_format template
9. Return to spawner

**Success criteria (`<success_criteria>`):**
Challenge generation complete when:
- [ ] All checkpoint-specific categories evaluated
- [ ] Every challenge has severity + concern + evidence + affected
- [ ] At least one challenge surfaced (nothing is perfect)
- [ ] Constructive tone used throughout ("Potential risk..." not "This is wrong")
- [ ] If round > 1: Defense Assessment table populated
- [ ] Convergence recommendation included with rationale
- [ ] Output follows structured format for spawner parsing
  </action>
  <verify>
Verify convergence and guardrail sections:
```bash
# Check convergence logic
grep -c "<convergence_logic>" agents/gsd-adversary.md
grep -c "CONTINUE" agents/gsd-adversary.md
grep -c "CONVERGE" agents/gsd-adversary.md

# Check anti-pattern sections
grep -c "<anti_sycophancy>" agents/gsd-adversary.md
grep -c "<anti_gridlock>" agents/gsd-adversary.md

# Check defense assessment
grep -c "<defense_assessment>" agents/gsd-adversary.md
grep -c "addressed" agents/gsd-adversary.md
grep -c "rejected" agents/gsd-adversary.md

# Check execution process
grep -c "<execution_process>" agents/gsd-adversary.md

# Check success criteria
grep -c "<success_criteria>" agents/gsd-adversary.md

# Verify total file size (should be substantial)
wc -l agents/gsd-adversary.md
```
  </verify>
  <done>
Agent file completed with:
- Convergence logic (CONTINUE vs CONVERGE signals)
- Defense assessment process for multi-round debates
- Anti-sycophancy guardrails (prevent rubber-stamping)
- Anti-gridlock guardrails (prevent blocking everything)
- Step-by-step execution process
- Success criteria checklist
- File is 200+ lines with comprehensive coverage
  </done>
</task>

</tasks>

<verification>
After both tasks complete, verify the agent is ready:

```bash
# Structure verification
test -f agents/gsd-adversary.md
head -10 agents/gsd-adversary.md | grep "name: gsd-adversary"

# Section completeness
for section in role input_format checkpoint_challenges severity_classification \
               challenge_template output_format convergence_logic defense_assessment \
               anti_sycophancy anti_gridlock execution_process success_criteria; do
  grep -c "<$section>" agents/gsd-adversary.md && echo "$section: present"
done

# Content verification
grep -q "BLOCKING" agents/gsd-adversary.md && echo "Severity: BLOCKING present"
grep -q "MAJOR" agents/gsd-adversary.md && echo "Severity: MAJOR present"
grep -q "MINOR" agents/gsd-adversary.md && echo "Severity: MINOR present"
grep -q "Potential risk" agents/gsd-adversary.md && echo "Constructive tone present"
grep -q "Requirements checkpoint" agents/gsd-adversary.md && echo "Requirements checkpoint present"
grep -q "Roadmap checkpoint" agents/gsd-adversary.md && echo "Roadmap checkpoint present"
grep -q "Plan checkpoint" agents/gsd-adversary.md && echo "Plan checkpoint present"
grep -q "Verification checkpoint" agents/gsd-adversary.md && echo "Verification checkpoint present"

# Size check (should be 200+ lines)
lines=$(wc -l < agents/gsd-adversary.md)
[ "$lines" -ge 200 ] && echo "Size OK: $lines lines" || echo "WARNING: Only $lines lines"
```
</verification>

<success_criteria>
Phase 1 complete when:

1. `agents/gsd-adversary.md` exists with valid GSD agent structure
2. Agent can be spawned via Task tool (has proper frontmatter)
3. All four checkpoint types have specific challenge categories
4. Severity classification (BLOCKING/MAJOR/MINOR) is defined and used
5. Constructive tone is established ("Potential risk..." phrasing)
6. Convergence recommendation logic is complete
7. Anti-sycophancy and anti-gridlock guardrails are present
8. Output format is structured for orchestrator parsing

Requirements covered:
- AGENT-01: Constructive stance (in role + challenge template)
- AGENT-02: Severity classification (in severity_classification section)
- AGENT-03: Checkpoint adaptation (in checkpoint_challenges section)
- CONV-02: Adaptive convergence (in convergence_logic section)
</success_criteria>

<output>
After completion, create `.planning/phases/01-core-agent/01-01-SUMMARY.md`
</output>
